{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "folder = 'embeddings/ELMo/'\n",
    "\n",
    "with open(folder + \"trainelmo.pickle\", \"rb\") as pickle_in:\n",
    "    train_data = pickle.load(pickle_in)\n",
    "with open(folder + \"develmo.pickle\", \"rb\") as pickle_in:\n",
    "    dev_data = pickle.load(pickle_in)\n",
    "with open(folder + \"testelmo.pickle\", \"rb\") as pickle_in:\n",
    "    test_data = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3871, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "names = [\"OBJECT A\", \"OBJECT B\", \"ASPECT\", \"MOST FREQUENT RATING\", \"SENTENCE\"]\n",
    "df_train = pd.read_csv(\"classification_fine_grained/train_clf_fine_grained.csv\", header=None, names=names)\n",
    "df_test = pd.read_csv(\"classification_fine_grained/test_clf_fine_grained.csv\", header=None, names=names)\n",
    "df_dev = pd.read_csv(\"classification_fine_grained/dev_clf_fine_grained.csv\", header=None, names=names)\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping duplicate object-aspect combinations  \n",
    "ONLY RUN IF YOU ARE NOT GOING TO USE SENTENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1206"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_dups(data, df):\n",
    "    df.drop('SENTENCE', axis=1, inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    new_data = []\n",
    "    for i in range(len(df.index)):\n",
    "        new_data.append(data[df.index[i]])\n",
    "    return new_data\n",
    "\n",
    "train_data = drop_dups(train_data, df_train)\n",
    "dev_data = drop_dups(dev_data, df_dev)\n",
    "test_data = drop_dups(test_data, df_test)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train 3871\n",
      "len dev 461\n",
      "len test 608\n"
     ]
    }
   ],
   "source": [
    "print(\"len train \" + str(len(train_data)))\n",
    "print(\"len dev \" + str(len(dev_data)))\n",
    "print(\"len test \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences under token limit(25): 0.9609919917334022\n"
     ]
    }
   ],
   "source": [
    "token_limit = 25\n",
    "\n",
    "under_limit = 0\n",
    "for i in range(len(train_data)):\n",
    "    if len(train_data[i][8]) <= token_limit:\n",
    "        under_limit += 1\n",
    "under_limit /= len(train_data)\n",
    "print('sentences under token limit(' + str(token_limit) + '): ' + str(under_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling empty embeddings with zeros (in cases w2v does not contain the words mentioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train 3871\n",
      "len dev 461\n",
      "len test 608\n"
     ]
    }
   ],
   "source": [
    "def fill_empty(data):\n",
    "    embedding_dim = 300\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(5, 9):\n",
    "            if len(data[i][j]) == 0:\n",
    "                data[i][j].append(np.zeros(embedding_dim))\n",
    "        new_data.append(data[i])\n",
    "    return new_data\n",
    "\n",
    "train_data = fill_empty(train_data)\n",
    "dev_data = fill_empty(dev_data)\n",
    "test_data = fill_empty(test_data)\n",
    "print(\"len train \" + str(len(train_data)))\n",
    "print(\"len dev \" + str(len(dev_data)))\n",
    "print(\"len test \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting samples with no embeddings (another way to treat empty embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train 3743\n",
      "len dev 451\n",
      "len test 559\n"
     ]
    }
   ],
   "source": [
    "def delete_empty(data):\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        ok = True\n",
    "        for j in range(5, 9):\n",
    "            if len(data[i][j]) == 0:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            new_data.append(data[i])\n",
    "    return new_data\n",
    "\n",
    "train_data = delete_empty(train_data)\n",
    "dev_data = delete_empty(dev_data)\n",
    "test_data = delete_empty(test_data)\n",
    "print(\"len train \" + str(len(train_data)))\n",
    "print(\"len dev \" + str(len(dev_data)))\n",
    "print(\"len test \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_for_binary(data):\n",
    "    out = []\n",
    "#     new_data = []\n",
    "    for i in range(len(data)):\n",
    "#         new_data.append(data[i])\n",
    "#         if random.choice([True, False]):\n",
    "#             new_data[i][5] = data[i][6]\n",
    "#             new_data[i][6] = data[i][5]\n",
    "#             out.append(1)\n",
    "#         else:\n",
    "#             out.append(0)\n",
    "        if data[i][3] == 'BAD':\n",
    "            out.append(0)\n",
    "        else:\n",
    "            out.append(1)\n",
    "    return out\n",
    "\n",
    "y_bin_train = get_output_for_binary(train_data)\n",
    "y_bin_dev = get_output_for_binary(dev_data)\n",
    "y_bin_test = get_output_for_binary(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples number:\n",
      "train:\t3871\n",
      "dev:\t461\n",
      "test:\t608\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples number:\")\n",
    "print(\"train:\\t\" + str(len(y_bin_train)))\n",
    "print(\"dev:\\t\" + str(len(y_bin_dev)))\n",
    "print(\"test:\\t\" + str(len(y_bin_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_for_multi(data):\n",
    "    out = []\n",
    "#     new_data = []\n",
    "    for i in range(len(data)):\n",
    "#         new_data.append(data[i])\n",
    "#         if random.choice([True, False]):\n",
    "#             new_data[i][5] = data[i][6]\n",
    "#             new_data[i][6] = data[i][5]\n",
    "#             out.append(1)\n",
    "#         else:\n",
    "#             out.append(0)\n",
    "        if data[i][3] == 'BAD':\n",
    "            out.append(0)\n",
    "        elif data[i][3] == 'ASPECT':\n",
    "            out.append(1)\n",
    "        elif data[i][3] == 'PREDICATE-FULL':\n",
    "            out.append(2)\n",
    "        elif data[i][3] == 'PREDICATE-DEPENDENT':\n",
    "            out.append(3)\n",
    "    return out\n",
    "\n",
    "y_multi_train = get_output_for_multi(train_data)\n",
    "y_multi_dev = get_output_for_multi(dev_data)\n",
    "y_multi_test = get_output_for_multi(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples number:\n",
      "train:\t3871\n",
      "dev:\t461\n",
      "test:\t608\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples number:\")\n",
    "print(\"train:\\t\" + str(len(y_multi_train)))\n",
    "print(\"dev:\\t\" + str(len(y_multi_dev)))\n",
    "print(\"test:\\t\" + str(len(y_multi_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_embedding(embeddings):\n",
    "    avg = embeddings[0]\n",
    "    for i in range(1, len(embeddings)):\n",
    "        avg = [avg[j] + embeddings[i][j] for j in range(len(avg))]\n",
    "    for i in range(len(avg)):\n",
    "        avg[i] /= len(embeddings)\n",
    "    return avg\n",
    "\n",
    "def max_pooling(embeddings):\n",
    "    max = embeddings[0]\n",
    "    for i in range(1, len(embeddings)):\n",
    "        for j in range(len(avg)):\n",
    "            if max[j] < embeddings[i][j]:\n",
    "                max[j] = embeddings[i][j]\n",
    "    return max\n",
    "\n",
    "def get_input_vectors(data):\n",
    "    vectors = []\n",
    "    max_tokens = 26\n",
    "    for i in range(len(data)):\n",
    "#         if data[i][4].find(data[i][0]) > data[i][4].find(data[i][1]):\n",
    "#             first_object = np.array([1], dtype='float32')\n",
    "#         else:\n",
    "#             first_object = np.array([0], dtype='float32')\n",
    "#         object_a = get_avg_embedding(data[i][5])\n",
    "#         object_b = get_avg_embedding(data[i][6])\n",
    "#         aspect = get_avg_embedding(data[i][7])\n",
    "#         sentence = get_avg_embedding(data[i][8])\n",
    "#         if len(data[i][8]) < max_tokens:\n",
    "#             sentence = np.concatenate(data[i][8])\n",
    "#             to_add = max_tokens - len(data[i][8])\n",
    "#             for j in range(to_add):\n",
    "#                 sentence = np.concatenate((sentence, np.zeros(300)))\n",
    "#         else:\n",
    "#             sentence = np.concatenate(data[i][8][:max_tokens])\n",
    "#         vectors.append(np.concatenate((object_a, object_b, aspect)))\n",
    "        object_a = data[i][5]\n",
    "        object_b = data[i][6]\n",
    "        aspect = data[i][7]\n",
    "        sentence = data[i][8]\n",
    "        vectors.append(np.concatenate((object_a, object_b, aspect, sentence)))\n",
    "#         vectors.append(np.array([object_a, object_b, aspect, sentence]).mean(axis=0))\n",
    "#         vectors.append(np.concatenate((first_object, sentence)))\n",
    "    return vectors\n",
    "\n",
    "X_train = get_input_vectors(train_data)\n",
    "X_dev = get_input_vectors(dev_data)\n",
    "X_test = get_input_vectors(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check classes balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.532937\n",
       "1    0.467063\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_bin_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.532937\n",
       "2    0.253940\n",
       "3    0.126066\n",
       "1    0.087058\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_multi_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def report_scores(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    pr = precision_score(y, y_pred, average='weighted')\n",
    "    re = recall_score(y, y_pred, average='weighted')\n",
    "    f1 = f1_score(y, y_pred, average='weighted')\n",
    "    print(\"Accuracy: {:.2f}\".format(acc * 100))\n",
    "    print(\"Precision: {:.2f}\".format(pr * 100))\n",
    "    print(\"Recall: {:.2f}\".format(re * 100))\n",
    "    print(\"F1: {:.2f}\".format(f1 * 100))\n",
    "    \n",
    "    \n",
    "def report_scores_multi(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    pr = precision_score(y, y_pred, average='weighted')\n",
    "    re = recall_score(y, y_pred, average='weighted')\n",
    "    f1 = f1_score(y, y_pred, average='weighted')\n",
    "    print(\"Accuracy: {:.2f}\".format(acc * 100))\n",
    "    print(\"Precision: {:.2f}\".format(pr * 100))\n",
    "    print(\"Recall: {:.2f}\".format(re * 100))\n",
    "    print(\"F1: {:.2f}\".format(f1 * 100))\n",
    "    target_names = ['BAD', 'ASPECT', 'PREDICATE-FULL', 'PREDICATE-DEPENDENT']\n",
    "    print(classification_report(y, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 98.22\n",
      "Precision: 98.25\n",
      "Recall: 98.22\n",
      "F1: 98.22\n",
      "Dev\n",
      "Accuracy: 84.38\n",
      "Precision: 85.63\n",
      "Recall: 84.38\n",
      "F1: 84.37\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, max_depth=9, min_samples_leaf=1)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_bin_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_bin_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 81.74\n",
      "Precision: 82.67\n",
      "Recall: 81.74\n",
      "F1: 81.62\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_bin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=0, test_size=0.2,\n",
       "            train_size=None),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='warn', n_jobs=4,\n",
       "             param_grid={'max_depth': range(3, 10),\n",
       "                         'min_samples_leaf': range(1, 10, 2),\n",
       "                         'n_estimators': [10, 20, 30, 50, 80, 100, 200]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "# print(model.get_params().keys())\n",
    "parameters_grid = {\n",
    "    'n_estimators': [10, 20, 30, 50, 80, 100, 200],\n",
    "    # 'min_samples_split': range(11, 17),\n",
    "    'min_samples_leaf': range(1, 10, 2),\n",
    "    'max_depth': range(3, 10)\n",
    "    }\n",
    "\n",
    "cv = StratifiedShuffleSplit(test_size=0.2, random_state=0)\n",
    "grid_cv = GridSearchCV(model, parameters_grid, scoring='accuracy', cv=cv, n_jobs=4)\n",
    "\n",
    "grid_cv.fit(X_train, y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=9, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583225806451613"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9, 'min_samples_leaf': 1, 'n_estimators': 200}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 97.83\n",
      "Precision: 97.84\n",
      "Recall: 97.83\n",
      "F1: 97.83\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.98      0.99      0.98      2063\n",
      "             ASPECT       1.00      0.95      0.98       337\n",
      "     PREDICATE-FULL       0.97      0.97      0.97       983\n",
      "PREDICATE-DEPENDENT       0.99      0.97      0.98       488\n",
      "\n",
      "           accuracy                           0.98      3871\n",
      "          macro avg       0.98      0.97      0.98      3871\n",
      "       weighted avg       0.98      0.98      0.98      3871\n",
      "\n",
      "Dev\n",
      "Accuracy: 75.27\n",
      "Precision: 75.01\n",
      "Recall: 75.27\n",
      "F1: 73.23\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.73      0.94      0.82       213\n",
      "             ASPECT       0.62      0.16      0.25        32\n",
      "     PREDICATE-FULL       0.85      0.72      0.78       167\n",
      "PREDICATE-DEPENDENT       0.58      0.43      0.49        49\n",
      "\n",
      "           accuracy                           0.75       461\n",
      "          macro avg       0.70      0.56      0.59       461\n",
      "       weighted avg       0.75      0.75      0.73       461\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_depth=9, min_samples_leaf=1)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_multi_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores_multi(model, X_train, y_multi_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores_multi(model, X_dev, y_multi_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 75.82\n",
      "Precision: 77.27\n",
      "Recall: 75.82\n",
      "F1: 73.76\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.74      0.93      0.82       303\n",
      "             ASPECT       1.00      0.31      0.47        65\n",
      "     PREDICATE-FULL       0.85      0.79      0.82       162\n",
      "PREDICATE-DEPENDENT       0.57      0.38      0.46        78\n",
      "\n",
      "           accuracy                           0.76       608\n",
      "          macro avg       0.79      0.60      0.64       608\n",
      "       weighted avg       0.77      0.76      0.74       608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores_multi(model, X_test, y_multi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 86.90\n",
      "Precision: 87.49\n",
      "Recall: 86.90\n",
      "F1: 86.78\n",
      "Dev\n",
      "Accuracy: 65.73\n",
      "Precision: 65.62\n",
      "Recall: 65.73\n",
      "F1: 65.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_bin_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_bin_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 76.81\n",
      "Precision: 77.37\n",
      "Recall: 76.81\n",
      "F1: 76.70\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_bin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['algorithm', 'leaf_size', 'metric', 'metric_params', 'n_jobs', 'n_neighbors', 'p', 'weights'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=0, test_size=0.2,\n",
       "            train_size=None),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                            metric='minkowski',\n",
       "                                            metric_params=None, n_jobs=None,\n",
       "                                            n_neighbors=5, p=2,\n",
       "                                            weights='uniform'),\n",
       "             iid='warn', n_jobs=None, param_grid={'n_neighbors': range(1, 11)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_grid = {\n",
    "    'n_neighbors': range(1, 11),\n",
    "    }\n",
    "\n",
    "cv = StratifiedShuffleSplit(test_size=0.2, random_state=0)\n",
    "grid_cv = GridSearchCV(model, parameters_grid, scoring='accuracy', cv=cv)\n",
    "\n",
    "grid_cv.fit(X_train, y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9660645161290322"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 1}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.90\n",
      "Precision: 45.73\n",
      "Recall: 55.90\n",
      "F1: 49.20\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.83      0.94      0.88      2063\n",
      "             ASPECT       0.14      0.66      0.24       337\n",
      "     PREDICATE-FULL       0.00      0.00      0.00       983\n",
      "PREDICATE-DEPENDENT       0.00      0.00      0.00       488\n",
      "\n",
      "           accuracy                           0.56      3871\n",
      "          macro avg       0.24      0.40      0.28      3871\n",
      "       weighted avg       0.46      0.56      0.49      3871\n",
      "\n",
      "Dev\n",
      "Accuracy: 28.20\n",
      "Precision: 29.75\n",
      "Recall: 28.20\n",
      "F1: 28.54\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.64      0.59      0.61       213\n",
      "             ASPECT       0.02      0.16      0.03        32\n",
      "     PREDICATE-FULL       0.00      0.00      0.00       167\n",
      "PREDICATE-DEPENDENT       0.00      0.00      0.00        49\n",
      "\n",
      "           accuracy                           0.28       461\n",
      "          macro avg       0.16      0.19      0.16       461\n",
      "       weighted avg       0.30      0.28      0.29       461\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores_multi(model, X_train, y_multi_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores_multi(model, X_dev, y_multi_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 46.71\n",
      "Precision: 37.81\n",
      "Recall: 46.71\n",
      "F1: 40.97\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.73      0.84      0.78       303\n",
      "             ASPECT       0.11      0.46      0.18        65\n",
      "     PREDICATE-FULL       0.00      0.00      0.00       162\n",
      "PREDICATE-DEPENDENT       0.00      0.00      0.00        78\n",
      "\n",
      "           accuracy                           0.47       608\n",
      "          macro avg       0.21      0.32      0.24       608\n",
      "       weighted avg       0.38      0.47      0.41       608\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores_multi(model, X_test, y_multi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=0, test_size=0.2,\n",
       "            train_size=None),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                            metric='minkowski',\n",
       "                                            metric_params=None, n_jobs=None,\n",
       "                                            n_neighbors=10, p=2,\n",
       "                                            weights='uniform'),\n",
       "             iid='warn', n_jobs=4, param_grid={'n_neighbors': range(1, 20)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_weighted', verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_grid = {\n",
    "    'n_neighbors': range(1, 20),\n",
    "    }\n",
    "\n",
    "cv = StratifiedShuffleSplit(test_size=0.2, random_state=0)\n",
    "grid_cv = GridSearchCV(model, parameters_grid, scoring='f1_weighted', cv=cv, n_jobs=4)\n",
    "\n",
    "grid_cv.fit(X_train, y_multi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9571938251800783"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 1}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 81.35\n",
      "Precision: 82.45\n",
      "Recall: 81.35\n",
      "F1: 81.03\n",
      "Dev\n",
      "Accuracy: 79.61\n",
      "Precision: 84.51\n",
      "Recall: 79.61\n",
      "F1: 79.21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel='rbf', gamma='auto')\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_bin_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_bin_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 77.80\n",
      "Precision: 80.42\n",
      "Recall: 77.80\n",
      "F1: 77.32\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_bin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.78\n",
      "Precision: 41.40\n",
      "Recall: 49.78\n",
      "F1: 45.07\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.77      0.92      0.84      2063\n",
      "             ASPECT       0.02      0.09      0.03       337\n",
      "     PREDICATE-FULL       0.00      0.00      0.00       983\n",
      "PREDICATE-DEPENDENT       0.00      0.00      0.00       488\n",
      "\n",
      "           accuracy                           0.50      3871\n",
      "          macro avg       0.20      0.25      0.22      3871\n",
      "       weighted avg       0.41      0.50      0.45      3871\n",
      "\n",
      "Dev\n",
      "Accuracy: 45.12\n",
      "Precision: 32.36\n",
      "Recall: 45.12\n",
      "F1: 37.69\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.70      0.98      0.82       213\n",
      "             ASPECT       0.00      0.00      0.00        32\n",
      "     PREDICATE-FULL       0.00      0.00      0.00       167\n",
      "PREDICATE-DEPENDENT       0.00      0.00      0.00        49\n",
      "\n",
      "           accuracy                           0.45       461\n",
      "          macro avg       0.18      0.24      0.20       461\n",
      "       weighted avg       0.32      0.45      0.38       461\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model = SVC(kernel='rbf', gamma='auto')\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores_multi(model, X_train, y_multi_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores_multi(model, X_dev, y_multi_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 46.88\n",
      "Precision: 35.84\n",
      "Recall: 46.88\n",
      "F1: 40.54\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.71      0.92      0.81       303\n",
      "             ASPECT       0.02      0.08      0.04        65\n",
      "     PREDICATE-FULL       0.00      0.00      0.00       162\n",
      "PREDICATE-DEPENDENT       0.00      0.00      0.00        78\n",
      "\n",
      "           accuracy                           0.47       608\n",
      "          macro avg       0.18      0.25      0.21       608\n",
      "       weighted avg       0.36      0.47      0.41       608\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores_multi(model, X_test, y_multi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 99.77\n",
      "F1: 99.77\n",
      "Dev\n",
      "Accuracy: 76.14\n",
      "F1: 76.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(solver='lbfgs', alpha=1e-3, hidden_layer_sizes=(10, 5, 3), random_state=1)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_bin_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_bin_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 82.73\n",
      "F1: 82.57\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_bin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 61.87\n",
      "F1: 32.80\n",
      "Dev\n",
      "Accuracy: 40.56\n",
      "F1: 21.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(solver='lbfgs', alpha=1e-3, hidden_layer_sizes=(10, 5, 3), random_state=1)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_multi_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_multi_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 50.82\n",
      "F1: 25.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_multi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
