{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'embeddings/W2V/'\n",
    "\n",
    "with open(folder + \"trainw2v.pickle\", \"rb\") as pickle_in:\n",
    "    train_data = pickle.load(pickle_in)\n",
    "with open(folder + \"devw2v.pickle\", \"rb\") as pickle_in:\n",
    "    dev_data = pickle.load(pickle_in)\n",
    "with open(folder + \"testw2v.pickle\", \"rb\") as pickle_in:\n",
    "    test_data = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3871, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [\"OBJECT A\", \"OBJECT B\", \"ASPECT\", \"MOST FREQUENT RATING\", \"SENTENCE\"]\n",
    "df_train = pd.read_csv(\"classification_fine_grained/train_clf_fine_grained.csv\", header=None, names=names)\n",
    "df_test = pd.read_csv(\"classification_fine_grained/test_clf_fine_grained.csv\", header=None, names=names)\n",
    "df_dev = pd.read_csv(\"classification_fine_grained/dev_clf_fine_grained.csv\", header=None, names=names)\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping duplicate object-aspect combinations  \n",
    "ONLY RUN IF YOU ARE NOT GOING TO USE SENTENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1206"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_dups(data, df):\n",
    "    df.drop('SENTENCE', axis=1, inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    new_data = []\n",
    "    for i in range(len(df.index)):\n",
    "        new_data.append(data[df.index[i]])\n",
    "    return new_data\n",
    "\n",
    "train_data = drop_dups(train_data, df_train)\n",
    "dev_data = drop_dups(dev_data, df_dev)\n",
    "test_data = drop_dups(test_data, df_test)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train 3871\n",
      "len dev 461\n",
      "len test 608\n"
     ]
    }
   ],
   "source": [
    "print(\"len train \" + str(len(train_data)))\n",
    "print(\"len dev \" + str(len(dev_data)))\n",
    "print(\"len test \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences under token limit(25): 0.9609919917334022\n"
     ]
    }
   ],
   "source": [
    "token_limit = 25\n",
    "\n",
    "under_limit = 0\n",
    "for i in range(len(train_data)):\n",
    "    if len(train_data[i][8]) <= token_limit:\n",
    "        under_limit += 1\n",
    "under_limit /= len(train_data)\n",
    "print('sentences under token limit(' + str(token_limit) + '): ' + str(under_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling empty embeddings with zeros (in cases w2v does not contain the words mentioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train 3871\n",
      "len dev 461\n",
      "len test 608\n"
     ]
    }
   ],
   "source": [
    "def fill_empty(data):\n",
    "    embedding_dim = 300\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(5, 9):\n",
    "            if len(data[i][j]) == 0:\n",
    "                data[i][j].append(np.zeros(embedding_dim))\n",
    "        new_data.append(data[i])\n",
    "    return new_data\n",
    "\n",
    "train_data = fill_empty(train_data)\n",
    "dev_data = fill_empty(dev_data)\n",
    "test_data = fill_empty(test_data)\n",
    "print(\"len train \" + str(len(train_data)))\n",
    "print(\"len dev \" + str(len(dev_data)))\n",
    "print(\"len test \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting samples with no embeddings (another way to treat empty embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train 3743\n",
      "len dev 451\n",
      "len test 559\n"
     ]
    }
   ],
   "source": [
    "def delete_empty(data):\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        ok = True\n",
    "        for j in range(5, 9):\n",
    "            if len(data[i][j]) == 0:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            new_data.append(data[i])\n",
    "    return new_data\n",
    "\n",
    "train_data = delete_empty(train_data)\n",
    "dev_data = delete_empty(dev_data)\n",
    "test_data = delete_empty(test_data)\n",
    "print(\"len train \" + str(len(train_data)))\n",
    "print(\"len dev \" + str(len(dev_data)))\n",
    "print(\"len test \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_for_binary(data):\n",
    "    out = []\n",
    "#     new_data = []\n",
    "    for i in range(len(data)):\n",
    "#         new_data.append(data[i])\n",
    "#         if random.choice([True, False]):\n",
    "#             new_data[i][5] = data[i][6]\n",
    "#             new_data[i][6] = data[i][5]\n",
    "#             out.append(1)\n",
    "#         else:\n",
    "#             out.append(0)\n",
    "        if data[i][3] == 'BAD':\n",
    "            out.append(0)\n",
    "        else:\n",
    "            out.append(1)\n",
    "    return out\n",
    "\n",
    "y_bin_train = get_output_for_binary(train_data)\n",
    "y_bin_dev = get_output_for_binary(dev_data)\n",
    "y_bin_test = get_output_for_binary(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples number:\n",
      "train:\t3871\n",
      "dev:\t461\n",
      "test:\t608\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples number:\")\n",
    "print(\"train:\\t\" + str(len(y_bin_train)))\n",
    "print(\"dev:\\t\" + str(len(y_bin_dev)))\n",
    "print(\"test:\\t\" + str(len(y_bin_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_for_multi(data):\n",
    "    out = []\n",
    "#     new_data = []\n",
    "    for i in range(len(data)):\n",
    "#         new_data.append(data[i])\n",
    "#         if random.choice([True, False]):\n",
    "#             new_data[i][5] = data[i][6]\n",
    "#             new_data[i][6] = data[i][5]\n",
    "#             out.append(1)\n",
    "#         else:\n",
    "#             out.append(0)\n",
    "        if data[i][3] == 'BAD':\n",
    "            out.append(0)\n",
    "        elif data[i][3] == 'ASPECT':\n",
    "            out.append(1)\n",
    "        elif data[i][3] == 'PREDICATE-FULL':\n",
    "            out.append(2)\n",
    "        elif data[i][3] == 'PREDICATE-DEPENDENT':\n",
    "            out.append(3)\n",
    "    return out\n",
    "\n",
    "y_multi_train = get_output_for_multi(train_data)\n",
    "y_multi_dev = get_output_for_multi(dev_data)\n",
    "y_multi_test = get_output_for_multi(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples number:\n",
      "train:\t3871\n",
      "dev:\t461\n",
      "test:\t608\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples number:\")\n",
    "print(\"train:\\t\" + str(len(y_multi_train)))\n",
    "print(\"dev:\\t\" + str(len(y_multi_dev)))\n",
    "print(\"test:\\t\" + str(len(y_multi_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_embedding(embeddings):\n",
    "    avg = embeddings[0]\n",
    "    for i in range(1, len(embeddings)):\n",
    "        avg = [avg[j] + embeddings[i][j] for j in range(len(avg))]\n",
    "    for i in range(len(avg)):\n",
    "        avg[i] /= len(embeddings)\n",
    "    return avg\n",
    "\n",
    "def max_pooling(embeddings):\n",
    "    max = embeddings[0]\n",
    "    for i in range(1, len(embeddings)):\n",
    "        for j in range(len(avg)):\n",
    "            if max[j] < embeddings[i][j]:\n",
    "                max[j] = embeddings[i][j]\n",
    "    return max\n",
    "\n",
    "def get_input_vectors(data):\n",
    "    vectors = []\n",
    "    max_tokens = 26\n",
    "    for i in range(len(data)):\n",
    "#         if data[i][4].find(data[i][0]) > data[i][4].find(data[i][1]):\n",
    "#             first_object = np.array([1], dtype='float32')\n",
    "#         else:\n",
    "#             first_object = np.array([0], dtype='float32')\n",
    "        object_a = get_avg_embedding(data[i][5])\n",
    "        object_b = get_avg_embedding(data[i][6])\n",
    "        aspect = get_avg_embedding(data[i][7])\n",
    "        sentence = get_avg_embedding(data[i][8])\n",
    "#         if len(data[i][8]) < max_tokens:\n",
    "#             sentence = np.concatenate(data[i][8])\n",
    "#             to_add = max_tokens - len(data[i][8])\n",
    "#             for j in range(to_add):\n",
    "#                 sentence = np.concatenate((sentence, np.zeros(300)))\n",
    "#         else:\n",
    "#             sentence = np.concatenate(data[i][8][:max_tokens])\n",
    "#         vectors.append(np.concatenate((object_a, object_b, aspect)))\n",
    "#         object_a = data[i][5]\n",
    "#         object_b = data[i][6]\n",
    "#         aspect = data[i][7]\n",
    "#         sentence = data[i][8]\n",
    "        vectors.append(np.concatenate((object_a, object_b, aspect, sentence)))\n",
    "#         vectors.append(np.array([object_a, object_b, aspect, sentence]).mean(axis=0))\n",
    "#         vectors.append(np.concatenate((first_object, sentence)))\n",
    "    return vectors\n",
    "\n",
    "X_train = get_input_vectors(train_data)\n",
    "X_dev = get_input_vectors(dev_data)\n",
    "X_test = get_input_vectors(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check classes balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.532937\n",
       "1    0.467063\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_bin_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.532937\n",
       "2    0.253940\n",
       "3    0.126066\n",
       "1    0.087058\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_multi_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_scores(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred, average='macro')\n",
    "    print(\"Accuracy: {:.2f}\".format(acc * 100))\n",
    "    print(\"F1: {:.2f}\".format(f1 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 97.75\n",
      "F1: 97.75\n",
      "Dev\n",
      "Accuracy: 88.94\n",
      "F1: 88.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=80, max_depth=9)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_bin_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_bin_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 81.91\n",
      "F1: 81.82\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_bin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=0, test_size=0.2,\n",
       "            train_size=None),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=4,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=30, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'max_depth': range(3, 10),\n",
       "                         'n_estimators': [10, 20, 30, 50, 80, 100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "\n",
    "# print(model.get_params().keys())\n",
    "parameters_grid = {\n",
    "    'n_estimators': [10, 20, 30, 50, 80, 100],\n",
    "    # 'criterion': ['gini', 'entropy'],\n",
    "    # 'min_samples_split': range(11, 17),\n",
    "    # 'min_samples_leaf': range(1, 10),\n",
    "    'max_depth': range(3, 10)\n",
    "    }\n",
    "\n",
    "cv = StratifiedShuffleSplit(test_size=0.2, random_state=0)\n",
    "grid_cv = GridSearchCV(model, parameters_grid, scoring='accuracy', cv=cv)\n",
    "\n",
    "grid_cv.fit(X_train, y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=9, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9532903225806452"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9, 'n_estimators': 80}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 97.83\n",
      "F1: 97.61\n",
      "Dev\n",
      "Accuracy: 78.31\n",
      "F1: 61.01\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=80, max_depth=9)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_multi_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_multi_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_multi_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 75.99\n",
      "F1: 65.41\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_multi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 86.95\n",
      "F1: 86.72\n",
      "Dev\n",
      "Accuracy: 65.73\n",
      "F1: 65.27\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_bin_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_bin_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 76.81\n",
      "F1: 76.70\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_bin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['algorithm', 'leaf_size', 'metric', 'metric_params', 'n_jobs', 'n_neighbors', 'p', 'weights'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=0, test_size=0.2,\n",
       "            train_size=None),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                            metric='minkowski',\n",
       "                                            metric_params=None, n_jobs=None,\n",
       "                                            n_neighbors=5, p=2,\n",
       "                                            weights='uniform'),\n",
       "             iid='warn', n_jobs=None, param_grid={'n_neighbors': range(1, 11)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_grid = {\n",
    "    'n_neighbors': range(1, 11),\n",
    "    }\n",
    "\n",
    "cv = StratifiedShuffleSplit(test_size=0.2, random_state=0)\n",
    "grid_cv = GridSearchCV(model, parameters_grid, scoring='accuracy', cv=cv)\n",
    "\n",
    "grid_cv.fit(X_train, y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9660645161290322"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 1}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Accuracy: 55.90\n",
      "F1: 28.04\n",
      "Dev\n",
      "Accuracy: 28.20\n",
      "F1: 16.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_multi_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_multi_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 46.71\n",
      "F1: 24.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_multi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 81.35\n",
      "F1: 80.82\n",
      "Dev\n",
      "Accuracy: 79.61\n",
      "F1: 79.38\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel='rbf', gamma='auto')\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_bin_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_bin_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 77.80\n",
      "F1: 77.33\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_bin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.78\n",
      "F1: 21.86\n",
      "Dev\n",
      "Accuracy: 45.12\n",
      "F1: 20.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model = SVC(kernel='rbf', gamma='auto')\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_multi_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_multi_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 46.88\n",
      "F1: 21.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_multi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 99.77\n",
      "F1: 99.77\n",
      "Dev\n",
      "Accuracy: 76.14\n",
      "F1: 76.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(solver='lbfgs', alpha=1e-3, hidden_layer_sizes=(10, 5, 3), random_state=1)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_bin_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_bin_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 82.73\n",
      "F1: 82.57\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_bin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 61.87\n",
      "F1: 32.80\n",
      "Dev\n",
      "Accuracy: 40.56\n",
      "F1: 21.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(solver='lbfgs', alpha=1e-3, hidden_layer_sizes=(10, 5, 3), random_state=1)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_multi_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_multi_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 50.82\n",
      "F1: 25.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_multi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
