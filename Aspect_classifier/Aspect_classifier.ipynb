{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_type = 'elmo' #w2v or elmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if embeddings_type == 'w2v':\n",
    "    folder = 'embeddings/W2V/'\n",
    "    with open(folder + \"trainw2v.pickle\", \"rb\") as pickle_in:\n",
    "        train_data = pickle.load(pickle_in)\n",
    "    with open(folder + \"devw2v.pickle\", \"rb\") as pickle_in:\n",
    "        dev_data = pickle.load(pickle_in)\n",
    "    with open(folder + \"testw2v.pickle\", \"rb\") as pickle_in:\n",
    "        test_data = pickle.load(pickle_in)\n",
    "    with open(folder + \"test_manual_binw2v.pickle\", \"rb\") as pickle_in:\n",
    "        test_manual_bin_data = pickle.load(pickle_in)\n",
    "    with open(folder + \"test_manual_multiw2v.pickle\", \"rb\") as pickle_in:\n",
    "        test_manual_multi_data = pickle.load(pickle_in)\n",
    "elif embeddings_type == 'elmo':\n",
    "    folder = 'embeddings/ELMo/'\n",
    "    with open(folder + \"trainelmo.pickle\", \"rb\") as pickle_in:\n",
    "        train_data = pickle.load(pickle_in)\n",
    "    with open(folder + \"develmo.pickle\", \"rb\") as pickle_in:\n",
    "        dev_data = pickle.load(pickle_in)\n",
    "    with open(folder + \"testelmo.pickle\", \"rb\") as pickle_in:\n",
    "        test_data = pickle.load(pickle_in)\n",
    "    with open(folder + \"test_manual_binelmo.pickle\", \"rb\") as pickle_in:\n",
    "        test_manual_bin_data = pickle.load(pickle_in)\n",
    "    with open(folder + \"test_manual_multielmo.pickle\", \"rb\") as pickle_in:\n",
    "        test_manual_multi_data = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3871, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "names = [\"OBJECT A\", \"OBJECT B\", \"ASPECT\", \"MOST FREQUENT RATING\", \"SENTENCE\"]\n",
    "df_train = pd.read_csv(\"classification_fine_grained/train_clf_fine_grained.csv\", header=None, names=names)\n",
    "df_test = pd.read_csv(\"classification_fine_grained/test_clf_fine_grained.csv\", header=None, names=names)\n",
    "df_dev = pd.read_csv(\"classification_fine_grained/dev_clf_fine_grained.csv\", header=None, names=names)\n",
    "df_test_manual_bin = pd.read_csv(\"classification_binary/test_manual_clf_binary.csv\", header=None, names=names)\n",
    "df_test_manual_multi = pd.read_csv(\"classification_fine_grained/test_manual_clf_fine_grained.csv\", header=None, names=names)\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping duplicate object-aspect combinations  \n",
    "ONLY RUN IF YOU ARE NOT GOING TO USE SENTENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def drop_dups(data, df):\n",
    "#     df.drop('SENTENCE', axis=1, inplace=True)\n",
    "#     df.drop_duplicates(inplace=True)\n",
    "#     new_data = []\n",
    "#     for i in range(len(df.index)):\n",
    "#         new_data.append(data[df.index[i]])\n",
    "#     return new_data\n",
    "\n",
    "# train_data = drop_dups(train_data, df_train)\n",
    "# dev_data = drop_dups(dev_data, df_dev)\n",
    "# test_data = drop_dups(test_data, df_test)\n",
    "# len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train 3871\n",
      "len dev 461\n",
      "len test 608\n",
      "len test manual bin 736\n",
      "len test manual multi 757\n"
     ]
    }
   ],
   "source": [
    "print(\"len train \" + str(len(train_data)))\n",
    "print(\"len dev \" + str(len(dev_data)))\n",
    "print(\"len test \" + str(len(test_data)))\n",
    "print(\"len test manual bin \" + str(len(test_manual_bin_data)))\n",
    "print(\"len test manual multi \" + str(len(test_manual_multi_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences under token limit(25): 0.0\n"
     ]
    }
   ],
   "source": [
    "token_limit = 25\n",
    "\n",
    "under_limit = 0\n",
    "for i in range(len(train_data)):\n",
    "    if len(train_data[i][8]) <= token_limit:\n",
    "        under_limit += 1\n",
    "under_limit /= len(train_data)\n",
    "print('sentences under token limit(' + str(token_limit) + '): ' + str(under_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling empty embeddings with zeros (in cases w2v does not contain the words mentioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train 3871\n",
      "len dev 461\n",
      "len test 608\n",
      "len test manual bin 736\n",
      "len test manual multi 757\n"
     ]
    }
   ],
   "source": [
    "def fill_empty(data):\n",
    "    embedding_dim = 300\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(5, 9):\n",
    "            if len(data[i][j]) == 0:\n",
    "                data[i][j].append(np.zeros(embedding_dim))\n",
    "        new_data.append(data[i])\n",
    "    return new_data\n",
    "\n",
    "train_data = fill_empty(train_data)\n",
    "dev_data = fill_empty(dev_data)\n",
    "test_data = fill_empty(test_data)\n",
    "test_manual_bin_data = fill_empty(test_manual_bin_data)\n",
    "test_manual_multi_data = fill_empty(test_manual_multi_data)\n",
    "print(\"len train \" + str(len(train_data)))\n",
    "print(\"len dev \" + str(len(dev_data)))\n",
    "print(\"len test \" + str(len(test_data)))\n",
    "print(\"len test manual bin \" + str(len(test_manual_bin_data)))\n",
    "print(\"len test manual multi \" + str(len(test_manual_multi_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting samples with no embeddings (another way to treat empty embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def delete_empty(data):\n",
    "#     new_data = []\n",
    "#     for i in range(len(data)):\n",
    "#         ok = True\n",
    "#         for j in range(5, 9):\n",
    "#             if len(data[i][j]) == 0:\n",
    "#                 ok = False\n",
    "#                 break\n",
    "#         if ok:\n",
    "#             new_data.append(data[i])\n",
    "#     return new_data\n",
    "\n",
    "# train_data = delete_empty(train_data)\n",
    "# dev_data = delete_empty(dev_data)\n",
    "# test_data = delete_empty(test_data)\n",
    "# test_manual_bin_data = delete_empty(test_manual_bin_data)\n",
    "# test_manual_multi_data = delete_empty(test_manual_multi_data)\n",
    "# print(\"len train \" + str(len(train_data)))\n",
    "# print(\"len dev \" + str(len(dev_data)))\n",
    "# print(\"len test \" + str(len(test_data)))\n",
    "# print(\"len test manual bin \" + str(len(test_manual_bin_data)))\n",
    "# print(\"len test manual multi \" + str(len(test_manual_multi_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_for_binary(data):\n",
    "    out = []\n",
    "#     new_data = []\n",
    "    for i in range(len(data)):\n",
    "#         new_data.append(data[i])\n",
    "#         if random.choice([True, False]):\n",
    "#             new_data[i][5] = data[i][6]\n",
    "#             new_data[i][6] = data[i][5]\n",
    "#             out.append(1)\n",
    "#         else:\n",
    "#             out.append(0)\n",
    "        if data[i][3] == 'BAD':\n",
    "            out.append(0)\n",
    "        else:\n",
    "            out.append(1)\n",
    "    return out\n",
    "\n",
    "y_bin_train = get_output_for_binary(train_data)\n",
    "y_bin_dev = get_output_for_binary(dev_data)\n",
    "y_bin_test = get_output_for_binary(test_data)\n",
    "y_bin_test_manual = get_output_for_binary(test_manual_bin_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples number:\n",
      "train:\t3871\n",
      "dev:\t461\n",
      "test:\t608\n",
      "test manual:\t736\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples number:\")\n",
    "print(\"train:\\t\" + str(len(y_bin_train)))\n",
    "print(\"dev:\\t\" + str(len(y_bin_dev)))\n",
    "print(\"test:\\t\" + str(len(y_bin_test)))\n",
    "print(\"test manual:\\t\" + str(len(y_bin_test_manual)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_for_multi(data):\n",
    "    out = []\n",
    "#     new_data = []\n",
    "    for i in range(len(data)):\n",
    "#         new_data.append(data[i])\n",
    "#         if random.choice([True, False]):\n",
    "#             new_data[i][5] = data[i][6]\n",
    "#             new_data[i][6] = data[i][5]\n",
    "#             out.append(1)\n",
    "#         else:\n",
    "#             out.append(0)\n",
    "        if data[i][3] == 'BAD':\n",
    "            out.append(0)\n",
    "        elif data[i][3] == 'ASPECT':\n",
    "            out.append(1)\n",
    "        elif data[i][3] == 'PREDICATE-FULL':\n",
    "            out.append(2)\n",
    "        elif data[i][3] == 'PREDICATE-DEPENDENT':\n",
    "            out.append(3)\n",
    "    return out\n",
    "\n",
    "y_multi_train = get_output_for_multi(train_data)\n",
    "y_multi_dev = get_output_for_multi(dev_data)\n",
    "y_multi_test = get_output_for_multi(test_data)\n",
    "y_multi_test_manual = get_output_for_multi(test_manual_multi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples number:\n",
      "train:\t3871\n",
      "dev:\t461\n",
      "test:\t608\n",
      "test manual:\t757\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples number:\")\n",
    "print(\"train:\\t\" + str(len(y_multi_train)))\n",
    "print(\"dev:\\t\" + str(len(y_multi_dev)))\n",
    "print(\"test:\\t\" + str(len(y_multi_test)))\n",
    "print(\"test manual:\\t\" + str(len(y_multi_test_manual)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_embedding(embeddings):\n",
    "    avg = embeddings[0]\n",
    "    for i in range(1, len(embeddings)):\n",
    "        avg = [avg[j] + embeddings[i][j] for j in range(len(avg))]\n",
    "    for i in range(len(avg)):\n",
    "        avg[i] /= len(embeddings)\n",
    "    return avg\n",
    "\n",
    "def max_pooling(embeddings):\n",
    "    max = embeddings[0]\n",
    "    for i in range(1, len(embeddings)):\n",
    "        for j in range(len(avg)):\n",
    "            if max[j] < embeddings[i][j]:\n",
    "                max[j] = embeddings[i][j]\n",
    "    return max\n",
    "\n",
    "def get_input_vectors(data):\n",
    "    vectors = []\n",
    "    max_tokens = 26\n",
    "    for i in range(len(data)):\n",
    "        if embeddings_type == 'w2v':\n",
    "            object_a = get_avg_embedding(data[i][5])\n",
    "            object_b = get_avg_embedding(data[i][6])\n",
    "            aspect = get_avg_embedding(data[i][7])\n",
    "            sentence = get_avg_embedding(data[i][8])\n",
    "        elif embeddings_type == 'elmo':\n",
    "            object_a = data[i][5]\n",
    "            object_b = data[i][6]\n",
    "            aspect = data[i][7]\n",
    "            sentence = data[i][8]\n",
    "#         if data[i][4].find(data[i][0]) > data[i][4].find(data[i][1]):\n",
    "#             first_object = np.array([1], dtype='float32')\n",
    "#         else:\n",
    "#             first_object = np.array([0], dtype='float32')\n",
    "#         if len(data[i][8]) < max_tokens:\n",
    "#             sentence = np.concatenate(data[i][8])\n",
    "#             to_add = max_tokens - len(data[i][8])\n",
    "#             for j in range(to_add):\n",
    "#                 sentence = np.concatenate((sentence, np.zeros(300)))\n",
    "#         else:\n",
    "#             sentence = np.concatenate(data[i][8][:max_tokens])\n",
    "#         vectors.append(np.concatenate((object_a, object_b, aspect)))\n",
    "        vectors.append(np.concatenate((object_a, object_b, aspect, sentence)))\n",
    "#         vectors.append(np.array([object_a, object_b, aspect, sentence]).mean(axis=0))\n",
    "#         vectors.append(np.concatenate((first_object, sentence)))\n",
    "    return vectors\n",
    "\n",
    "X_train = get_input_vectors(train_data)\n",
    "X_dev = get_input_vectors(dev_data)\n",
    "X_test = get_input_vectors(test_data)\n",
    "X_test_manual_bin = get_input_vectors(test_manual_bin_data)\n",
    "X_test_manual_multi = get_input_vectors(test_manual_multi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check classes balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.532937\n",
       "1    0.467063\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_bin_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.532937\n",
       "2    0.253940\n",
       "3    0.126066\n",
       "1    0.087058\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_multi_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_bin_test_manual).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.587847\n",
       "1    0.233818\n",
       "3    0.178336\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_multi_test_manual).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def report_scores(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    pr = precision_score(y, y_pred, average='weighted')\n",
    "    re = recall_score(y, y_pred, average='weighted')\n",
    "    f1 = f1_score(y, y_pred, average='weighted')\n",
    "    print(\"Accuracy: {:.2f}\".format(acc * 100))\n",
    "    print(\"Precision: {:.2f}\".format(pr * 100))\n",
    "    print(\"Recall: {:.2f}\".format(re * 100))\n",
    "    print(\"F1: {:.2f}\".format(f1 * 100))\n",
    "    \n",
    "    \n",
    "def report_scores_multi(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    pr = precision_score(y, y_pred, average='weighted')\n",
    "    re = recall_score(y, y_pred, average='weighted')\n",
    "    f1 = f1_score(y, y_pred, average='weighted')\n",
    "    print(\"Accuracy: {:.2f}\".format(acc * 100))\n",
    "    print(\"Precision: {:.2f}\".format(pr * 100))\n",
    "    print(\"Recall: {:.2f}\".format(re * 100))\n",
    "    print(\"F1: {:.2f}\".format(f1 * 100))\n",
    "    target_names = ['BAD', 'ASPECT', 'PREDICATE-FULL', 'PREDICATE-DEPENDENT']\n",
    "    print(classification_report(y, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 98.14\n",
      "Precision: 98.18\n",
      "Recall: 98.14\n",
      "F1: 98.14\n",
      "Dev\n",
      "Accuracy: 85.25\n",
      "Precision: 86.19\n",
      "Recall: 85.25\n",
      "F1: 85.25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, max_depth=9, min_samples_leaf=1)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_bin_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_bin_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 82.73\n",
      "Precision: 83.54\n",
      "Recall: 82.73\n",
      "F1: 82.63\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_bin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test manual\n",
      "Accuracy: 45.92\n",
      "Precision: 100.00\n",
      "Recall: 45.92\n",
      "F1: 62.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test manual\")\n",
    "report_scores(model, X_test_manual_bin, y_bin_test_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=0, test_size=0.2,\n",
       "            train_size=None),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='warn', n_jobs=4,\n",
       "             param_grid={'max_depth': range(3, 10),\n",
       "                         'min_samples_leaf': range(1, 10, 2),\n",
       "                         'n_estimators': [10, 20, 30, 50, 80, 100, 200]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "# print(model.get_params().keys())\n",
    "parameters_grid = {\n",
    "    'n_estimators': [10, 20, 30, 50, 80, 100, 200],\n",
    "    # 'min_samples_split': range(11, 17),\n",
    "    'min_samples_leaf': range(1, 10, 2),\n",
    "    'max_depth': range(3, 10)\n",
    "    }\n",
    "\n",
    "cv = StratifiedShuffleSplit(test_size=0.2, random_state=0)\n",
    "grid_cv = GridSearchCV(model, parameters_grid, scoring='accuracy', cv=cv, n_jobs=4)\n",
    "\n",
    "grid_cv.fit(X_train, y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=9, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583225806451613"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9, 'min_samples_leaf': 1, 'n_estimators': 200}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 98.81\n",
      "Precision: 98.83\n",
      "Recall: 98.81\n",
      "F1: 98.80\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.99      1.00      0.99      2063\n",
      "             ASPECT       1.00      0.92      0.96       337\n",
      "     PREDICATE-FULL       0.98      1.00      0.99       983\n",
      "PREDICATE-DEPENDENT       1.00      0.98      0.99       488\n",
      "\n",
      "           accuracy                           0.99      3871\n",
      "          macro avg       0.99      0.97      0.98      3871\n",
      "       weighted avg       0.99      0.99      0.99      3871\n",
      "\n",
      "Dev\n",
      "Accuracy: 75.05\n",
      "Precision: 74.08\n",
      "Recall: 75.05\n",
      "F1: 70.94\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.75      0.97      0.85       213\n",
      "             ASPECT       1.00      0.06      0.12        32\n",
      "     PREDICATE-FULL       0.84      0.80      0.82       167\n",
      "PREDICATE-DEPENDENT       0.19      0.10      0.13        49\n",
      "\n",
      "           accuracy                           0.75       461\n",
      "          macro avg       0.69      0.48      0.48       461\n",
      "       weighted avg       0.74      0.75      0.71       461\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_depth=9, min_samples_leaf=1)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_multi_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores_multi(model, X_train, y_multi_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores_multi(model, X_dev, y_multi_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 71.38\n",
      "Precision: 71.63\n",
      "Recall: 71.38\n",
      "F1: 67.18\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.73      0.91      0.81       303\n",
      "             ASPECT       1.00      0.15      0.27        65\n",
      "     PREDICATE-FULL       0.74      0.83      0.78       162\n",
      "PREDICATE-DEPENDENT       0.39      0.19      0.26        78\n",
      "\n",
      "           accuracy                           0.71       608\n",
      "          macro avg       0.71      0.52      0.53       608\n",
      "       weighted avg       0.72      0.71      0.67       608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores_multi(model, X_test, y_multi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test manual\n",
      "Accuracy: 28.93\n",
      "Precision: 81.19\n",
      "Recall: 28.93\n",
      "F1: 38.87\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.00      0.00      0.00         0\n",
      "             ASPECT       1.00      0.04      0.08       177\n",
      "     PREDICATE-FULL       0.82      0.44      0.58       445\n",
      "PREDICATE-DEPENDENT       0.54      0.11      0.18       135\n",
      "\n",
      "           accuracy                           0.29       757\n",
      "          macro avg       0.59      0.15      0.21       757\n",
      "       weighted avg       0.81      0.29      0.39       757\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test manual\")\n",
    "report_scores_multi(model, X_test_manual_multi, y_multi_test_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 89.36\n",
      "Precision: 89.55\n",
      "Recall: 89.36\n",
      "F1: 89.31\n",
      "Dev\n",
      "Accuracy: 80.48\n",
      "Precision: 80.51\n",
      "Recall: 80.48\n",
      "F1: 80.49\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_bin_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_bin_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 80.76\n",
      "Precision: 80.86\n",
      "Recall: 80.76\n",
      "F1: 80.74\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_bin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test manual\n",
      "Accuracy: 57.88\n",
      "Precision: 100.00\n",
      "Recall: 57.88\n",
      "F1: 73.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test manual\")\n",
    "report_scores(model, X_test_manual_bin, y_bin_test_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['algorithm', 'leaf_size', 'metric', 'metric_params', 'n_jobs', 'n_neighbors', 'p', 'weights'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=0, test_size=0.2,\n",
       "            train_size=None),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                            metric='minkowski',\n",
       "                                            metric_params=None, n_jobs=None,\n",
       "                                            n_neighbors=5, p=2,\n",
       "                                            weights='uniform'),\n",
       "             iid='warn', n_jobs=None, param_grid={'n_neighbors': range(1, 11)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_grid = {\n",
    "    'n_neighbors': range(1, 11),\n",
    "    }\n",
    "\n",
    "cv = StratifiedShuffleSplit(test_size=0.2, random_state=0)\n",
    "grid_cv = GridSearchCV(model, parameters_grid, scoring='accuracy', cv=cv)\n",
    "\n",
    "grid_cv.fit(X_train, y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9660645161290322"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 1}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 86.46\n",
      "Precision: 86.60\n",
      "Recall: 86.46\n",
      "F1: 86.09\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.87      0.95      0.91      2063\n",
      "             ASPECT       0.91      0.64      0.75       337\n",
      "     PREDICATE-FULL       0.84      0.85      0.85       983\n",
      "PREDICATE-DEPENDENT       0.88      0.69      0.77       488\n",
      "\n",
      "           accuracy                           0.86      3871\n",
      "          macro avg       0.87      0.78      0.82      3871\n",
      "       weighted avg       0.87      0.86      0.86      3871\n",
      "\n",
      "Dev\n",
      "Accuracy: 72.89\n",
      "Precision: 72.13\n",
      "Recall: 72.89\n",
      "F1: 72.43\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.77      0.81      0.79       213\n",
      "             ASPECT       0.19      0.19      0.19        32\n",
      "     PREDICATE-FULL       0.83      0.83      0.83       167\n",
      "PREDICATE-DEPENDENT       0.49      0.39      0.43        49\n",
      "\n",
      "           accuracy                           0.73       461\n",
      "          macro avg       0.57      0.55      0.56       461\n",
      "       weighted avg       0.72      0.73      0.72       461\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_multi_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores_multi(model, X_train, y_multi_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores_multi(model, X_dev, y_multi_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 68.91\n",
      "Precision: 68.12\n",
      "Recall: 68.91\n",
      "F1: 68.28\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.78      0.84      0.81       303\n",
      "             ASPECT       0.72      0.52      0.61        65\n",
      "     PREDICATE-FULL       0.67      0.69      0.68       162\n",
      "PREDICATE-DEPENDENT       0.27      0.24      0.26        78\n",
      "\n",
      "           accuracy                           0.69       608\n",
      "          macro avg       0.61      0.57      0.59       608\n",
      "       weighted avg       0.68      0.69      0.68       608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores_multi(model, X_test, y_multi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test manual\n",
      "Accuracy: 42.67\n",
      "Precision: 73.46\n",
      "Recall: 42.67\n",
      "F1: 52.02\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.00      0.00      0.00         0\n",
      "             ASPECT       0.87      0.22      0.35       177\n",
      "     PREDICATE-FULL       0.83      0.60      0.70       445\n",
      "PREDICATE-DEPENDENT       0.26      0.11      0.16       135\n",
      "\n",
      "           accuracy                           0.43       757\n",
      "          macro avg       0.49      0.23      0.30       757\n",
      "       weighted avg       0.73      0.43      0.52       757\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test manual\")\n",
    "report_scores_multi(model, X_test_manual_multi, y_multi_test_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=0, test_size=0.2,\n",
       "            train_size=None),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                            metric='minkowski',\n",
       "                                            metric_params=None, n_jobs=None,\n",
       "                                            n_neighbors=10, p=2,\n",
       "                                            weights='uniform'),\n",
       "             iid='warn', n_jobs=4, param_grid={'n_neighbors': range(1, 20)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_weighted', verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_grid = {\n",
    "    'n_neighbors': range(1, 20),\n",
    "    }\n",
    "\n",
    "cv = StratifiedShuffleSplit(test_size=0.2, random_state=0)\n",
    "grid_cv = GridSearchCV(model, parameters_grid, scoring='f1_weighted', cv=cv, n_jobs=4)\n",
    "\n",
    "grid_cv.fit(X_train, y_multi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9571938251800783"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 1}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 86.64\n",
      "Precision: 86.68\n",
      "Recall: 86.64\n",
      "F1: 86.65\n",
      "Dev\n",
      "Accuracy: 88.29\n",
      "Precision: 88.53\n",
      "Recall: 88.29\n",
      "F1: 88.30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel='rbf', gamma='auto')\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_bin_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_bin_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 83.22\n",
      "Precision: 83.25\n",
      "Recall: 83.22\n",
      "F1: 83.22\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_bin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test manual\n",
      "Accuracy: 61.82\n",
      "Precision: 100.00\n",
      "Recall: 61.82\n",
      "F1: 76.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test manual\")\n",
    "report_scores(model, X_test_manual_bin, y_bin_test_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 82.64\n",
      "Precision: 83.54\n",
      "Recall: 82.64\n",
      "F1: 81.30\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.86      0.92      0.89      2063\n",
      "             ASPECT       1.00      0.36      0.53       337\n",
      "     PREDICATE-FULL       0.77      0.93      0.84       983\n",
      "PREDICATE-DEPENDENT       0.76      0.55      0.64       488\n",
      "\n",
      "           accuracy                           0.83      3871\n",
      "          macro avg       0.85      0.69      0.72      3871\n",
      "       weighted avg       0.84      0.83      0.81      3871\n",
      "\n",
      "Dev\n",
      "Accuracy: 77.01\n",
      "Precision: 70.74\n",
      "Recall: 77.01\n",
      "F1: 72.61\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.75      0.95      0.83       213\n",
      "             ASPECT       0.00      0.00      0.00        32\n",
      "     PREDICATE-FULL       0.83      0.84      0.84       167\n",
      "PREDICATE-DEPENDENT       0.57      0.24      0.34        49\n",
      "\n",
      "           accuracy                           0.77       461\n",
      "          macro avg       0.54      0.51      0.50       461\n",
      "       weighted avg       0.71      0.77      0.73       461\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model = SVC(kernel='rbf', gamma='auto')\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_multi_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores_multi(model, X_train, y_multi_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores_multi(model, X_dev, y_multi_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 73.03\n",
      "Precision: 73.28\n",
      "Recall: 73.03\n",
      "F1: 70.12\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.75      0.91      0.82       303\n",
      "             ASPECT       1.00      0.28      0.43        65\n",
      "     PREDICATE-FULL       0.74      0.81      0.77       162\n",
      "PREDICATE-DEPENDENT       0.45      0.24      0.32        78\n",
      "\n",
      "           accuracy                           0.73       608\n",
      "          macro avg       0.73      0.56      0.59       608\n",
      "       weighted avg       0.73      0.73      0.70       608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores_multi(model, X_test, y_multi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test manual\n",
      "Accuracy: 36.86\n",
      "Precision: 78.75\n",
      "Recall: 36.86\n",
      "F1: 44.64\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                BAD       0.00      0.00      0.00         0\n",
      "             ASPECT       1.00      0.07      0.14       177\n",
      "     PREDICATE-FULL       0.79      0.58      0.67       445\n",
      "PREDICATE-DEPENDENT       0.50      0.07      0.13       135\n",
      "\n",
      "           accuracy                           0.37       757\n",
      "          macro avg       0.57      0.18      0.23       757\n",
      "       weighted avg       0.79      0.37      0.45       757\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test manual\")\n",
    "report_scores_multi(model, X_test_manual_multi, y_multi_test_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 99.77\n",
      "F1: 99.77\n",
      "Dev\n",
      "Accuracy: 76.14\n",
      "F1: 76.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(solver='lbfgs', alpha=1e-3, hidden_layer_sizes=(10, 5, 3), random_state=1)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_bin_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_bin_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 82.73\n",
      "F1: 82.57\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_bin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of fit\n",
      "Train\n",
      "Accuracy: 61.87\n",
      "F1: 32.80\n",
      "Dev\n",
      "Accuracy: 40.56\n",
      "F1: 21.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(solver='lbfgs', alpha=1e-3, hidden_layer_sizes=(10, 5, 3), random_state=1)\n",
    "\n",
    "print(\"start of fit\")\n",
    "model.fit(X_train, y_bin_train)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train\")\n",
    "report_scores(model, X_train, y_multi_train)\n",
    "\n",
    "print(\"Dev\")\n",
    "report_scores(model, X_dev, y_multi_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy: 50.82\n",
      "F1: 25.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Test\")\n",
    "report_scores(model, X_test, y_multi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
